"""Module to orchestrate all interaction with the OpenAI completions endpoint through a structured Completion object"""
from __future__ import annotations

import asyncio
import json
import logging
import os
import random
from abc import ABC
from abc import abstractmethod
from collections import OrderedDict
from time import time
from typing import Any

import openai
import tiktoken

from api.utils import post_request
from api.utils import Response
from constants import DEFAULT_TIMEOUTS
from constants import GLOBAL_TIMEOUT
from constants import OPENAI_DV3_ORG
from constants import OPENAI_MODEL_FALLBACKS
from conversation.utils import compute_word_logprobs

logger = logging.getLogger(__name__)

BASE_API_URL = os.environ.get("OPENAI_URL", "https://api.openai.com")
HEADERS = {"Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}"}


class CompletionType:
    TEXT = "text"
    CHAT = "chat"


class Completion(ABC):
    """Completion object uses a prompt, completion primer, and completion parameters to query an LM and generate
    a completion.

    Attributes
    ----------
    prompt : str
        full text of the prompt that will be fed to a LM for completion
    completion_primer : str
        predetermined part of the completion that is appended to the prompt to nudge the completion in
        the correct direction
    messages : list[dict[str, str]]
        full list of messages in {role: user|assistant, content: text} format
    model : str
        the name of the OpenAI model that is being queries; e.g. text-davinci-003
    params : Dict[str, Any]
        a set of completion parameters used by the LM to generate a completion
    completion_response : Optional[CompletionResponse]
        holds a completion response once generated
    num_generations : int
        represents the number of times the LM was queried in order to generate the full completion
    completion_type : Enum[CompletionType]
        TEXT or CHAT
    """

    prompt: str
    completion_primer: str
    messages: list[dict[str, str]]
    model: str
    params: dict[str, Any]
    completion_response: CompletionResponse | None = None
    num_generations: int = 0
    completion_type: str = CompletionType.TEXT

    def __init__(
        self, prompt: str, completion_primer: str = "", settings: dict[str, Any] = None
    ):
        """Create a Completion object with the requisite components to generate a Language Model completion.

        Parameters
        ----------
        prompt : str
            Full text of the prompt that will be fed to a LM for completion
        completion_primer : str
            Predetermined part of the completion that is appended to the prompt to nudge the completion in
            the correct direction
        settings : Dict[str, Any]
            Full set of completion parameters that affect the completion generated by the LM
        """
        self.prompt = prompt
        self.completion_primer = completion_primer
        self.params = self.parse_settings(settings or {})
        self.model = self.params["model"]

    @staticmethod
    def create(
        prompt: str,
        completion_primer: str = "",
        messages: list[str] = None,
        settings: dict[str, Any] = None,
    ) -> TextCompletion:
        completion_type = Completion.get_completion_type(settings)
        if completion_type == CompletionType.TEXT:
            return TextCompletion(
                prompt, completion_primer=completion_primer, settings=settings
            )
        elif completion_type == CompletionType.CHAT:
            return ChatCompletion(
                prompt,
                messages=messages,
                completion_primer=completion_primer,
                settings=settings,
            )
        else:
            raise ValueError(f"Invalid completion type: {completion_type}")

    @staticmethod
    def get_completion_type(settings: dict[str, Any] = None) -> str:
        """Return the completion type based on the settings."""
        return settings.get("completion_type", CompletionType.TEXT)

    def parse_settings(self, settings: dict[str, Any]) -> dict[str, Any]:
        return {
            param: settings.get(param, default)
            for param, default in self.default_params.items()
        }

    @abstractmethod
    def prepare_request_body(self) -> dict[str, Any]:
        pass

    @staticmethod
    async def _get_completion_with_retries(
        url: str,
        data: dict[str, Any],
        headers: dict[str, str],
        activity_id: str,
        turn_id: str,
        completion_type: str,
        timeout: float = GLOBAL_TIMEOUT,
        max_retries: int = 3,
        retry_delay: int = 1,
    ) -> tuple[CompletionResponse | None, int]:
        """Query the GPT Completions API up to max_retries times in case of request failure with a jitter of 1-2s"""
        for retry in range(max_retries):
            response = await post_request(url, data, headers, timeout=timeout)

            if "error" in response.data and response.data["error"]["type"] in {
                "server_error",
                "tokens",
                "requests",
                "TimeoutError",
            }:
                await asyncio.sleep(retry_delay + random.random())
                logger.info(
                    f"activityId={activity_id} :: turnId={turn_id} :: GPT-3 API call failed with error: "
                    f"{response.data['error']['message']} on attempt number {retry + 1}."
                )
            else:
                data = response.data["choices"][0]
                text = (
                    data["message"]["content"]
                    if completion_type == CompletionType.CHAT
                    else data["text"]
                )
                return (
                    CompletionResponse(
                        text=text,
                        message=data.get("message"),
                        finish_reason=data["finish_reason"],
                        usage=response.data["usage"],
                        logprobs=data.get("logprobs"),
                        latency=float(response.headers.get("Openai-Processing-Ms")),
                    ),
                    retry,
                )

    @staticmethod
    async def log_completion(
        request: dict[str, Any],
        completion_type: str,
        completion_response: CompletionResponse,
        model: str,
        max_tokens: int,
        latency: float,
        prompt: str,
        num_retries: int,
        model_fallback_from: str = None,
        stream: bool = False,
        activity_id: str = None,
        turn_id: str = None,
        user_id: str = None,
        call_type: str = None,
    ) -> None:

        # CAREFUL: used for log-based metrics!
        prompt_plus_max_tokens = completion_response.num_prompt_tokens + max_tokens
        logger.info(f"GPT3 USAGE:{completion_response.num_total_tokens}")
        logger.info(
            f"activityId={activity_id} :: turnId={turn_id} :: "
            f"GPT-3 API call ({model}) completed in "
            f"<latency:{latency:.2f}ms> <maxTokens:{max_tokens}> "
            f"<promptPlusMaxTokens:{prompt_plus_max_tokens}> "
            f"Full Request:\n{json.dumps(request, ensure_ascii=False, indent=4)}\n\n"
            f"API Input:\n{prompt}\n\nOutput:\n{completion_response.text}"
        )

    async def _generate_single_completion(
        self,
        activity_id: str = None,
        turn_id: str = None,
        user_id: str = None,
        call_type: str = None,
    ) -> CompletionResponse | None:
        """Query the GPT-3 Completions API"""
        t0 = time()

        # prepare completions request
        data = self.prepare_request_body()
        model = data["model"]
        headers = HEADERS.copy()
        timeout = DEFAULT_TIMEOUTS.get(call_type, GLOBAL_TIMEOUT)

        # CAREFUL: used for log-based metrics!
        logger.info(
            f"activityId={activity_id} :: turnId={turn_id} :: "
            f"GPT-3 API call parameters ({model}): { {**self.params, 'model': model} }"
        )

        # get completion
        try:
            completion_response, num_retries = await self._get_completion_with_retries(
                self.api_url,
                data,
                headers,
                activity_id,
                turn_id,
                self.completion_type,
                timeout=timeout,
            )
        except Exception as e:
            logger.error(f"Could not parse response from Completion with error: {e}")
            return None

        await self.log_completion(
            request=data,
            completion_type=self.completion_type,
            completion_response=completion_response,
            model=model,
            model_fallback_from=self.model if self.model != model else None,
            max_tokens=self.get_param("max_tokens"),
            latency=(time() - t0) * 1000,
            prompt=self.get_full_prompt(),
            num_retries=num_retries,
            activity_id=activity_id,
            turn_id=turn_id,
            user_id=user_id,
            call_type=call_type,
        )

        return completion_response

    async def generate_completion(
        self,
        activity_id: str = None,
        turn_id: str = None,
        user_id: str = None,
        call_type: str = None,
        finish_reason: str = "stop",
        max_generations: int = 10,
    ):
        """Requery the completion endpoint up to N times until the finish reason is reached. Even if the max_tokens
        is too low and the completion is truncated due to length, we can query again."""
        for i in range(max_generations):

            # get completion
            completion_response = await self._generate_single_completion(
                activity_id=activity_id,
                turn_id=turn_id,
                user_id=user_id,
                call_type=call_type,
            )
            if completion_response is None:
                logger.error(
                    f"activityId={activity_id} :: turnId={turn_id} :: "
                    f"Completion failed to generate; returning empty string"
                )
                break

            if self.completion_response:
                self.completion_response.update(completion_response)
            else:
                self.completion_response = completion_response
            self.num_generations += 1

            # if finish reason isn't `stop` or we've hit max_generations, add the completion to the context and requery the endpoint
            if (
                completion_response.finish_reason == finish_reason
                or i == max_generations - 1
            ):
                break
            self.completion_primer += completion_response.text

    def get_full_prompt(self) -> str:
        return self.prompt + self.completion_primer

    def get_model(self) -> str:
        return self.model

    def get_model_with_fallback(self) -> str:
        return self.model

    def get_params(self) -> dict[str, Any]:
        return self.params

    def get_param(self, param: str) -> Any | None:
        return self.params.get(param)

    def get_completion(self) -> str:
        text = self.completion_response.text if self.completion_response else ""
        return text.strip()

    def get_full_completion(self) -> str:
        text = self.completion_response.text if self.completion_response else ""
        return (self.completion_primer + text).strip()

    def get_parsed_tags(
        self,
        tags: list[str],
        format: str = None,
        activity_id: str = None,
        turn_id: str = None,
    ) -> OrderedDict[str, str]:
        completion = self.get_full_completion()
        if format:
            completion = (
                completion.replace(f"```{format}", "").replace("```", "").strip()
            )
        return self.parse_tags_from_completion(completion, tags, activity_id, turn_id)

    @staticmethod
    def parse_tags_from_completion(
        completion: str, tags: list[str], activity_id: str = None, turn_id: str = None
    ) -> OrderedDict[str, str]:
        """Tags are parsed from a free-form string. There are a few basic assumptions:

                1. The tags are present in the string in order
                2. Each tag is followed by a colon
                3. Tag keys will be parsed exactly without any lowercasing, punctuation removal, etc.
                4. No tag will occur twice in the completion string

        Parameters
        ----------
        completion : str
            string containing tag keys and values
        tags : List[str]
            an ordered list of tag keys

        Returns
        -------
        OrderedDict[str, str]
            map of tag keys to parsed values
        """
        text = completion
        parsed_tags = OrderedDict()
        prev_tag = "dummy_tag"

        for tag in tags:
            try:
                prev_tag_value, text = text.split(tag + ":")
                parsed_tags[prev_tag] = prev_tag_value.strip()
                prev_tag = tag

            except:
                logger.info(
                    "activityId=%s :: turnId=%s :: Couldn't parse tag [%s] from completion: %s",
                    activity_id,
                    turn_id,
                    tag,
                    completion,
                )
                break

        parsed_tags[prev_tag] = text.strip()
        parsed_tags.pop("dummy_tag")

        return parsed_tags


class TextCompletion(Completion):
    """Completion object to format requests specifically for the text completions endpoint."""

    api_url: str = BASE_API_URL + "/v1/completions"
    default_params: dict[str, Any] = {
        "model": "text-davinci-002",
        "temperature": 0.9,
        "max_tokens": 100,
        "top_p": 1.0,
        "frequency_penalty": 1.0,
        "presence_penalty": 1.0,
        "stop": None,
        "logit_bias": {},
        "logprobs": 5,
    }

    def __init__(
        self, prompt: str, completion_primer: str = "", settings: dict[str, Any] = None
    ):
        super().__init__(prompt, completion_primer, settings)
        self.completion_type = CompletionType.TEXT

    def prepare_request_body(self) -> dict[str, Any]:
        return {
            "prompt": self.get_full_prompt(),
            **self.params,
            "model": self.get_model_with_fallback(),
        }


class ChatCompletion(Completion):
    """Completion object to format requests specifically for the chat completions endpoint."""

    api_url: str = BASE_API_URL + "/v1/chat/completions"
    default_params: dict[str, Any] = {
        "model": "gpt-3.5-turbo",
        "temperature": 0.9,
        "max_tokens": 150,
        "top_p": 1.0,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "stop": None,
        "logit_bias": {},
    }

    def __init__(
        self,
        prompt: str,
        messages: list[dict[str, str]] = None,
        completion_primer: str = "",
        settings: dict[str, Any] = None,
    ):
        super().__init__(prompt, completion_primer, settings)
        self.messages = messages or []
        self.completion_type = CompletionType.CHAT

    def get_system_message(self) -> str:
        return {
            "role": "system",
            "content": self.prompt,
        }

    def prepare_request_body(self):
        messages = (
            self.messages + [{"role": "assistant", "content": self.completion_primer}]
            if self.completion_primer
            else self.messages
        )
        return {
            "messages": [self.get_system_message(), *messages],
            **self.params,
            "model": self.get_model_with_fallback(),
        }

    @staticmethod
    def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613"):
        """Return the number of tokens used by a list of messages. Taken from the cookbook example at https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"""
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
        if model in {
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-16k-0613",
            "gpt-4-0314",
            "gpt-4-32k-0314",
            "gpt-4-0613",
            "gpt-4-32k-0613",
        }:
            tokens_per_message = 3
            tokens_per_name = 1
        elif model == "gpt-3.5-turbo-0301":
            tokens_per_message = (
                4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
            )
            tokens_per_name = -1  # if there's a name, the role is omitted
        elif "gpt-3.5-turbo" in model:
            return ChatCompletion.num_tokens_from_messages(
                messages, model="gpt-3.5-turbo-0613"
            )
        elif "gpt-4" in model:
            return ChatCompletion.num_tokens_from_messages(messages, model="gpt-4-0613")
        elif "gpt-dv-speak" in model:
            return ChatCompletion.num_tokens_from_messages(messages, model="gpt-4-0314")
        else:
            raise NotImplementedError(
                f"""num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."""
            )
        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    async def generate_streamed_completion(
        self,
        activity_id: str = None,
        turn_id: str = None,
        user_id: str = None,
        call_type: str = None,
        finish_reason: str = "stop",
        max_generations: int = 10,
    ):
        """Requery the completion endpoint up to N times until the finish reason is reached. Even if the max_tokens
        is too low and the completion is truncated due to length, we can query again."""
        role = ""
        generation_finish_reason = ""
        text_deltas = []

        for i in range(max_generations):
            t0 = time()

            data = self.prepare_request_body()
            model = data["model"]
            response = await openai.ChatCompletion.acreate(
                **data,
                stream=True,
            )

            # process streamed completion and collect metadata
            async for chunk in response:
                delta = chunk["choices"][0]["delta"]
                if "role" in delta:
                    role = delta["role"]
                elif "content" in delta:
                    text_deltas.append(delta["content"])
                    yield delta["content"]
                else:
                    generation_finish_reason = chunk["choices"][0]["finish_reason"]

            # construct completion response
            message = {
                "role": role,
                "content": "".join(text_deltas),
            }
            usage = {
                "prompt_tokens": self.num_tokens_from_messages(
                    [self.get_system_message(), *self.messages], self.model
                ),
                "completion_tokens": self.num_tokens_from_messages(
                    [message], self.model
                ),
            }
            completion_response = CompletionResponse(
                text=message["content"],
                message=message,
                finish_reason=generation_finish_reason,
                usage=usage,
                latency=(time() - t0) * 1000,
            )

            if completion_response is None:
                logger.error(
                    f"activityId={activity_id} :: turnId={turn_id} :: "
                    f"Completion failed to generate; returning empty string"
                )
                break

            await self.log_completion(
                request=self.prepare_request_body(),
                completion_type=self.completion_type,
                completion_response=completion_response,
                model=model,
                model_fallback_from=self.model if self.model != model else None,
                max_tokens=self.get_param("max_tokens"),
                latency=completion_response.latency,
                prompt=self.get_full_prompt(),
                num_retries=1,
                stream=True,
                activity_id=activity_id,
                turn_id=turn_id,
                user_id=user_id,
                call_type=call_type,
            )

            if self.completion_response:
                self.completion_response.update(completion_response)
            else:
                self.completion_response = completion_response
            self.num_generations += 1

            # if finish reason isn't `stop` or we've hit max_generations, add the completion to the context and requery the endpoint
            if (
                completion_response.finish_reason == finish_reason
                or i == max_generations - 1
            ):
                break
            self.completion_primer += completion_response.text


class CompletionResponse:
    """Contains text completion and all api metadata returned from completion endpoint

    Attributes
    ----------
    text : str
        raw text of the generated completion
    finish_reason : str
        reason for stopping the completion: e.g. `stop`, `length`, etc.
    token_logprobs : List[Dict[str, Any]]
        log probabilities for all tokens in completion
    word_logprobs : List[Dict[str, Any]]
        log probabilities for all words in completion
    num_prompt_tokens : int
        total number of tokens in the submitted prompt
    num_completion_tokens : int
        total number of tokens in the generated completion
    num_total_tokens : int
        total number of tokens in the prompt and completion
    latency : float
        openai processing latency in ms
    """

    text: str
    message: dict[str, str]
    finish_reason: str
    token_logprobs: list[dict[str, Any]] = []
    word_logprobs: list[dict[str, Any]] = []
    num_prompt_tokens: int
    num_completion_tokens: int
    num_total_tokens: int
    latency: float

    def __init__(
        self,
        text: str,
        finish_reason: str,
        usage: dict[str, int],
        logprobs: dict[str, Any] = None,
        latency: float = None,
        message: dict[str, str] = None,
    ):

        self.text = text
        self.message = message
        self.finish_reason = finish_reason
        self.latency = latency
        self.parse_token_usage(usage)
        if logprobs:
            self.parse_logprobs(logprobs)

    def parse_logprobs(self, logprobs: dict[str, Any]):
        self.token_logprobs = [
            {"token": tok, "logprob": logprobs["token_logprobs"][i]}
            for i, tok in enumerate(logprobs["tokens"])
        ]
        self.word_logprobs = compute_word_logprobs(self.token_logprobs)

    def parse_token_usage(self, usage):
        self.num_prompt_tokens = usage.get("prompt_tokens", 0)
        self.num_completion_tokens = usage.get("completion_tokens", 0)
        self.num_total_tokens = usage.get(
            "total_tokens", self.num_prompt_tokens + self.num_completion_tokens
        )

    def update(self, other: CompletionResponse):
        self.text = other.text
        if self.message:
            self.message["content"] += other.message["content"]
        self.finish_reason = other.finish_reason
        self.latency += other.latency
        self.token_logprobs.extend(other.token_logprobs)
        self.word_logprobs.extend(other.word_logprobs)
        self.num_prompt_tokens += other.num_prompt_tokens
        self.num_completion_tokens += other.num_completion_tokens
        self.num_total_tokens += other.num_total_tokens
